{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convolutional variational autoencoder\n",
    "- generate a bunch of new ones\n",
    "- interpolate between emojis\n",
    "- maxpool <> depool\n",
    "- conv <> deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Dense, Flatten, Input, Layer as KerasLayer, Reshape\n",
    "from keras.losses import mean_squared_error, binary_crossentropy, mean_absolute_error\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from IPython.display import SVG\n",
    "import numpy as np\n",
    "import PIL\n",
    "from scipy.ndimage import imread\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMOJIS_DIR = 'data/emojis'\n",
    "N_CHANNELS = 4\n",
    "EMOJI_SHAPE = (36, 36, N_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try to add in emojis shaped (36, 36, 2), here, by adding empty axes\n",
    "\n",
    "emojis = []\n",
    "\n",
    "for slug in os.listdir(EMOJIS_DIR):\n",
    "    path = os.path.join(EMOJIS_DIR, slug)\n",
    "    emoji = imread(path)\n",
    "    if emoji.shape == (36, 36, 4):\n",
    "        emojis.append(emoji) \n",
    "\n",
    "emojis = np.array(emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = np.random.rand( len(emojis) ) < 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = y_train = emojis[train_mask] / 255.\n",
    "X_val = y_val = emojis[~train_mask] / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_emoji(emoji_arr):\n",
    "    return PIL.Image.fromarray(emoji_arr)\n",
    "\n",
    "\n",
    "def display_prediction(pred):\n",
    "    pred = (pred * 255).astype(np.uint8)\n",
    "    return display_emoji(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VariationalLayer(KerasLayer):\n",
    "\n",
    "    def __init__(self, embedding_dim: int, epsilon_std=1.):\n",
    "        '''A custom \"variational\" Keras layer that completes the\n",
    "        variational autoencoder.\n",
    "\n",
    "        Args:\n",
    "            embedding_dim : The desired number of latent dimensions in our\n",
    "                embedding space.\n",
    "        '''\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.epsilon_std = epsilon_std\n",
    "        super().__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.z_mean_weights = self.add_weight(\n",
    "            shape=input_shape[-1:] + (self.embedding_dim,),\n",
    "            initializer='glorot_normal',\n",
    "            trainable=True,\n",
    "            name='z_mean_weights'\n",
    "        )\n",
    "        self.z_mean_bias = self.add_weight(\n",
    "            shape=(self.embedding_dim,),\n",
    "            initializer='zero',\n",
    "            trainable=True,\n",
    "            name='z_mean_bias'\n",
    "        )\n",
    "        self.z_log_var_weights = self.add_weight(\n",
    "            shape=input_shape[-1:] + (self.embedding_dim,),\n",
    "            initializer='glorot_normal',\n",
    "            trainable=True,\n",
    "            name='z_log_var_weights'\n",
    "        )\n",
    "        self.z_log_var_bias = self.add_weight(\n",
    "            shape=(self.embedding_dim,),\n",
    "            initializer='zero',\n",
    "            trainable=True,\n",
    "            name='z_log_var_bias'\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        z_mean = K.dot(x, self.z_mean_weights) + self.z_mean_bias\n",
    "        z_log_var = K.dot(x, self.z_log_var_weights) + self.z_log_var_bias\n",
    "        epsilon = K.random_normal(\n",
    "            shape=K.shape(z_log_var),\n",
    "            mean=0.,\n",
    "            stddev=self.epsilon_std\n",
    "        )\n",
    "\n",
    "        kl_loss_numerator = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "        self.kl_loss = -0.5 * K.sum(kl_loss_numerator, axis=-1)\n",
    "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "    def loss(self, x, x_decoded):\n",
    "        base_loss = binary_crossentropy(x, x_decoded)\n",
    "        base_loss = tf.reduce_sum(base_loss, axis=[-1, -2])\n",
    "        return base_loss + self.kl_loss\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:1] + (self.embedding_dim,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 16\n",
    "FILTER_SIZE = 64\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encoder\n",
    "original = Input(shape=EMOJI_SHAPE, name='original')\n",
    "\n",
    "conv = Conv2D(filters=FILTER_SIZE, kernel_size=3, input_shape=original.shape, padding='same', activation='relu')(original)\n",
    "conv = Conv2D(filters=FILTER_SIZE, kernel_size=3,padding='same', activation='relu')(conv)\n",
    "conv = Conv2D(filters=FILTER_SIZE, kernel_size=3,padding='same', activation='relu')(conv)\n",
    "\n",
    "flat = Flatten()(conv)\n",
    "variational_layer = VariationalLayer(EMBEDDING_SIZE)\n",
    "variational_params = variational_layer(flat)\n",
    "\n",
    "encoder = Model([original], [variational_params], name='encoder')\n",
    "\n",
    "# decoder\n",
    "encoded = Input(shape=(EMBEDDING_SIZE,))\n",
    "\n",
    "upsample = Dense(np.multiply.reduce(EMOJI_SHAPE), activation='relu')(encoded)\n",
    "reshape = Reshape(EMOJI_SHAPE)(upsample)\n",
    "\n",
    "deconv = Conv2DTranspose(filters=FILTER_SIZE, kernel_size=3, padding='same', activation='relu', input_shape=encoded.shape)(reshape)\n",
    "deconv = Conv2DTranspose(filters=FILTER_SIZE, kernel_size=3, padding='same', activation='relu')(deconv)\n",
    "deconv = Conv2DTranspose(filters=FILTER_SIZE, kernel_size=3, padding='same', activation='relu')(deconv)\n",
    "reconstructed = Conv2DTranspose(filters=N_CHANNELS, kernel_size=3, padding='same', activation='sigmoid')(deconv)\n",
    "\n",
    "decoder = Model([encoded], [reconstructed], name='decoder')\n",
    "\n",
    "# end-to-end\n",
    "encoder_decoder = Model([original], decoder(encoder([original])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVG(model_to_dot(encoder_decoder).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_decoder.compile(optimizer=Adam(.001), loss=variational_layer.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96 samples, validate on 186 samples\n",
      "Epoch 1/1\n",
      "96/96 [==============================] - 8s - loss: 1910.4568 - val_loss: 882.7920\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "('Not JSON Serializable:', Dimension(None))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-2efc8bed7839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mencoder_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/vae.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/neurally-embedded-emojis/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   2504\u001b[0m         \"\"\"\n\u001b[1;32m   2505\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2506\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/neurally-embedded-emojis/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;34m'config'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     }, default=get_json_type).encode('utf8')\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/neurally-embedded-emojis/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         **kw).encode(obj)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/neurally-embedded-emojis/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/neurally-embedded-emojis/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m~/anaconda3/envs/neurally-embedded-emojis/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mget_json_type\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not JSON Serializable:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Not JSON Serializable:', Dimension(None))"
     ]
    }
   ],
   "source": [
    "encoder_decoder_fit = encoder_decoder.fit(\n",
    "    x=X_train[:96],\n",
    "    y=y_train[:96],\n",
    "    batch_size=16,\n",
    "    epochs=1,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "encoder_decoder.save('models/vae.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emj = X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = encoder_decoder.predict(np.array([emj]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prediction(emj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prediction(preds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# question-answer models for emoji responses\n",
    "- when predicting, you can predict onto the generated emojis as well!\n",
    "- add some dropout\n",
    "- vanishing/exploding gradients\n",
    "- spaCy for word2vec embeddings?\n",
    "\n",
    "## bi-directional lstms\n",
    "- \"Bidirectional Long Short-Term Memory (biLSTM): Single direction LSTMs suffer a weakness\n",
    "of not utilizing the contextual information from the future tokens. Bidirectional LSTM utilizes both\n",
    "the previous and future context by processing the sequence on two directions, and generate two\n",
    "independent sequences of LSTM output vectors. One processes the input sequence in the forward\n",
    "direction, while the other processes the input in the reverse direction. The output at each time step\n",
    "is the concatenation of the two output vectors from both directions, ie. ht =\n",
    "−→ht k\n",
    "←−ht .\" (https://arxiv.org/pdf/1511.04108.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# references\n",
    "- http://ben.bolte.cc/blog/2016/language.html\n",
    "- https://arxiv.org/pdf/1508.01585v2.pdf\n",
    "- https://arxiv.org/pdf/1511.04108.pdf\n",
    "- https://explosion.ai/blog/deep-learning-formula-nlp\n",
    "- https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
