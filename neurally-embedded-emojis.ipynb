{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convolutional variational autoencoder\n",
    "- generate a bunch of new ones\n",
    "- interpolate between emojis\n",
    "- maxpool <> depool\n",
    "- conv <> deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import concatenate, dot, merge\n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten, Input, Lambda\n",
    "from keras.layers import Bidirectional, Conv2D, Conv2DTranspose, LSTM, MaxPool1D\n",
    "from keras.layers import Layer as KerasLayer, Reshape\n",
    "from keras.losses import mean_squared_error, binary_crossentropy, mean_absolute_error\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.regularizers import l2\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from IPython.display import SVG\n",
    "from matplotlib import gridspec, pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "from scipy.ndimage import imread\n",
    "from sklearn.preprocessing import scale\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMOJIS_DIR = 'data/emojis'\n",
    "N_CHANNELS = 4\n",
    "EMOJI_SHAPE = (36, 36, N_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try to add in emojis shaped (36, 36, 2), here, by adding empty axes\n",
    "\n",
    "emojis_dict = {}\n",
    "\n",
    "for slug in os.listdir(EMOJIS_DIR):\n",
    "    path = os.path.join(EMOJIS_DIR, slug)\n",
    "    emoji = imread(path)\n",
    "    if emoji.shape == (36, 36, 2):\n",
    "        emoji = np.c_[emoji, np.zeros_like(emoji)]\n",
    "    emojis_dict[slug] = emoji\n",
    "\n",
    "emojis = np.array( list(emojis_dict.values()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = np.random.rand( len(emojis) ) < 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = y_train = emojis[train_mask] / 255.\n",
    "X_val = y_val = emojis[~train_mask] / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_emoji(emoji_arr):\n",
    "    return PIL.Image.fromarray(emoji_arr)\n",
    "\n",
    "\n",
    "def display_prediction(pred):\n",
    "    pred = (pred * 255).astype(np.uint8)\n",
    "    return display_emoji(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VariationalLayer(KerasLayer):\n",
    "\n",
    "    def __init__(self, embedding_dim: int, epsilon_std=1.):\n",
    "        '''A custom \"variational\" Keras layer that completes the\n",
    "        variational autoencoder.\n",
    "\n",
    "        Args:\n",
    "            embedding_dim : The desired number of latent dimensions in our\n",
    "                embedding space.\n",
    "        '''\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.epsilon_std = epsilon_std\n",
    "        super().__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.z_mean_weights = self.add_weight(\n",
    "            shape=input_shape[-1:] + (self.embedding_dim,),\n",
    "            initializer='glorot_normal',\n",
    "            trainable=True,\n",
    "            name='z_mean_weights'\n",
    "        )\n",
    "        self.z_mean_bias = self.add_weight(\n",
    "            shape=(self.embedding_dim,),\n",
    "            initializer='zero',\n",
    "            trainable=True,\n",
    "            name='z_mean_bias'\n",
    "        )\n",
    "        self.z_log_var_weights = self.add_weight(\n",
    "            shape=input_shape[-1:] + (self.embedding_dim,),\n",
    "            initializer='glorot_normal',\n",
    "            trainable=True,\n",
    "            name='z_log_var_weights'\n",
    "        )\n",
    "        self.z_log_var_bias = self.add_weight(\n",
    "            shape=(self.embedding_dim,),\n",
    "            initializer='zero',\n",
    "            trainable=True,\n",
    "            name='z_log_var_bias'\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        z_mean = K.dot(x, self.z_mean_weights) + self.z_mean_bias\n",
    "        z_log_var = K.dot(x, self.z_log_var_weights) + self.z_log_var_bias\n",
    "        epsilon = K.random_normal(\n",
    "            shape=K.shape(z_log_var),\n",
    "            mean=0.,\n",
    "            stddev=self.epsilon_std\n",
    "        )\n",
    "\n",
    "        kl_loss_numerator = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "        self.kl_loss = -0.5 * K.sum(kl_loss_numerator, axis=-1)\n",
    "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "    def loss(self, x, x_decoded):\n",
    "        base_loss = binary_crossentropy(x, x_decoded)\n",
    "        base_loss = tf.reduce_sum(base_loss, axis=[-1, -2])\n",
    "        return base_loss + self.kl_loss\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:1] + (self.embedding_dim,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 16\n",
    "FILTER_SIZE = 64\n",
    "BATCH_SIZE = 16\n",
    "WEIGHTS_PATH = 'weights/epoch_{epoch:02d}-loss_{val_loss:.2f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# probably add some dropout!\n",
    "\n",
    "# encoder\n",
    "original = Input(shape=EMOJI_SHAPE, name='original')\n",
    "\n",
    "conv = Conv2D(filters=FILTER_SIZE, kernel_size=3, input_shape=original.shape, padding='same', activation='relu')(original)\n",
    "conv = Conv2D(filters=FILTER_SIZE, kernel_size=3,padding='same', activation='relu')(conv)\n",
    "conv = Conv2D(filters=FILTER_SIZE, kernel_size=3,padding='same', activation='relu')(conv)\n",
    "\n",
    "flat = Flatten()(conv)\n",
    "variational_layer = VariationalLayer(EMBEDDING_SIZE)\n",
    "variational_params = variational_layer(flat)\n",
    "\n",
    "encoder = Model([original], [variational_params], name='encoder')\n",
    "\n",
    "# decoder\n",
    "encoded = Input(shape=(EMBEDDING_SIZE,))\n",
    "\n",
    "upsample = Dense(np.multiply.reduce(EMOJI_SHAPE), activation='relu')(encoded)\n",
    "reshape = Reshape(EMOJI_SHAPE)(upsample)\n",
    "\n",
    "deconv = Conv2DTranspose(filters=FILTER_SIZE, kernel_size=3, padding='same', activation='relu', input_shape=encoded.shape)(reshape)\n",
    "deconv = Conv2DTranspose(filters=FILTER_SIZE, kernel_size=3, padding='same', activation='relu')(deconv)\n",
    "deconv = Conv2DTranspose(filters=FILTER_SIZE, kernel_size=3, padding='same', activation='relu')(deconv)\n",
    "dropout = Dropout(.8)(deconv)\n",
    "reconstructed = Conv2DTranspose(filters=N_CHANNELS, kernel_size=3, padding='same', activation='sigmoid')(dropout)\n",
    "\n",
    "decoder = Model([encoded], [reconstructed], name='decoder')\n",
    "\n",
    "# end-to-end\n",
    "encoder_decoder = Model([original], decoder(encoder([original])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVG(model_to_dot(encoder_decoder).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_decoder.compile(optimizer=Adam(.003), loss=variational_layer.loss)\n",
    "encoder_decoder.load_weights('weights/overfitted_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.set_value(encoder_decoder.optimizer.lr, .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(WEIGHTS_PATH, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder_decoder_fit = encoder_decoder.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=16,\n",
    "    epochs=500,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# encoder_decoder.save_weights('weights/vae_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emj = emojis[189]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_emoji(emj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = encoder_decoder.predict(np.array([emj]) / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prediction(pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code_16d = Input(shape=(16,), name='original')\n",
    "# code_2d = Dense(2)(code_16d)\n",
    "# code_16d_ = Dense(16)(code_2d)\n",
    "\n",
    "# mini_autoencoder = Model([code_16d], [code_16d_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mini_autoencoder.compile(optimizer=Adam(.01), loss=mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini_autoencoder.fit(x=preds, y=preds, batch_size=BATCH_SIZE, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compose_code(first_half, second_half):\n",
    "    # change this up! go every other! have fun!\n",
    "    return 8 * [first_half] + 8 * [second_half]\n",
    "\n",
    "\n",
    "ticks = 20\n",
    "axis = np.linspace(-2, 2, ticks)\n",
    "\n",
    "\n",
    "linspace_codes = [compose_code(i, j) for i, j in product(axis, axis)]\n",
    "generated_emojis = decoder.predict(linspace_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = n_cols = ticks\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "gs = gridspec.GridSpec(n_rows, n_cols, wspace=.01, hspace=0)\n",
    "\n",
    "for i, (r, c) in enumerate(product(range(n_rows), range(n_cols))):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "    ax.imshow(generated_emojis[i])\n",
    "\n",
    "plt.suptitle('Generated Emojis')\n",
    "# plt.savefig('figures/features.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# question-answer models for emoji responses\n",
    "- when predicting, you can predict onto the generated emojis as well!\n",
    "- add some dropout\n",
    "- vanishing/exploding gradients\n",
    "- spaCy for word2vec embeddings?\n",
    "\n",
    "## bi-directional lstms\n",
    "- \"Bidirectional Long Short-Term Memory (biLSTM): Single direction LSTMs suffer a weakness\n",
    "of not utilizing the contextual information from the future tokens. Bidirectional LSTM utilizes both\n",
    "the previous and future context by processing the sequence on two directions, and generate two\n",
    "independent sequences of LSTM output vectors. One processes the input sequence in the forward\n",
    "direction, while the other processes the input in the reverse direction. The output at each time step\n",
    "is the concatenation of the two output vectors from both directions, ie. ht =\n",
    "−→ht k\n",
    "←−ht .\" (https://arxiv.org/pdf/1511.04108.pdf)\n",
    "- when discussing this thing, talk a bit about the jeremy howard course, returning the sequences, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text\n",
    "- tokens to word2vec vectors, maybe via spacy\n",
    "- embed into sentence matrix via bi-directional lstm\n",
    "- attention mechanism, i think (or maybe omit)\n",
    "- max-pool\n",
    "- dense-layer (same as above)\n",
    "- tanh\n",
    "- cosine\n",
    "\n",
    "# emoji\n",
    "- dense layer\n",
    "- concat with attention mechanism vector (or maybe omit)\n",
    "- max-pool\n",
    "- dense-layer (same as above)\n",
    "- tanh\n",
    "- cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import airlines tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12336</td>\n",
       "      <td>@AmericanAir watch out if you are a parent...t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10623</td>\n",
       "      <td>@USAirways you call this a window seat? #airbu...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2902</td>\n",
       "      <td>@united I'm familiar with the procedure. It wo...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9822</td>\n",
       "      <td>@USAirways it's flt 4827 to ELM - still no gat...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2885</td>\n",
       "      <td>@united Flight is awful only one lavatory func...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text airline_sentiment\n",
       "0  12336  @AmericanAir watch out if you are a parent...t...          negative\n",
       "1  10623  @USAirways you call this a window seat? #airbu...          negative\n",
       "2   2902  @united I'm familiar with the procedure. It wo...          negative\n",
       "3   9822  @USAirways it's flt 4827 to ELM - still no gat...          negative\n",
       "4   2885  @united Flight is awful only one lavatory func...          negative"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('data/tweets.csv')[['text', 'airline_sentiment']]\\\n",
    "    .sample(5000)\\\n",
    "    .reset_index()\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_decoder.load_weights('weights/epoch_01-loss_544.55.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_array = np.array([emojis_dict['1f389.png'], emojis_dict['1f608.png'], emojis_dict['1f621.png']])\n",
    "encodings = encoder.predict(labels_array).astype(np.float64)\n",
    "\n",
    "encodings = scale(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_emb, neutral_emb, negative_emb = encodings\n",
    "embedding_map = {'positive': positive_emb, 'neutral': neutral_emb, 'negative': negative_emb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_answers, correct_answers = [], []\n",
    "sentiments = {'positive', 'neutral', 'negative'}\n",
    "for sentiment in tweets_df['airline_sentiment']:\n",
    "    correct_answers.append( embedding_map[sentiment] )\n",
    "    incorrect_sentiment = random.sample(sentiments - {sentiment}, 1)[0]\n",
    "    incorrect_answers.append( embedding_map[incorrect_sentiment] )\n",
    "    \n",
    "\n",
    "questions = tweets_df['text']\n",
    "correct_answers = np.array(correct_answers)\n",
    "incorrect_answers = np.array(incorrect_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_QUESTION_LEN = 20\n",
    "VOCAB_SIZE = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(questions)\n",
    "question_seqs = tokenizer.texts_to_sequences(questions)\n",
    "question_seqs = pad_sequences(question_seqs, maxlen=MAX_QUESTION_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into train, validation\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "train_mask = np.random.rand( len(question_seqs) ) < 0.8\n",
    "\n",
    "questions_train = question_seqs[train_mask]\n",
    "correct_answers_train = correct_answers[train_mask]\n",
    "incorrect_answers_train = incorrect_answers[train_mask]\n",
    "\n",
    "questions_val = question_seqs[~train_mask]\n",
    "correct_answers_val = correct_answers[~train_mask]\n",
    "incorrect_answers_val = incorrect_answers[~train_mask]\n",
    "\n",
    "y_train_dummy = np.zeros(shape=questions_train.shape[0])\n",
    "y_val_dummy = np.zeros(shape=questions_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLOVE_EMBEDDINGS_DIR = 'data/glove.6B'\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# hydrate embedding layer\n",
    "# THIS CODE WAS TAKEN FROM https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "# francois: i'll try to make a pull request if i have time\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_EMBEDDINGS_DIR, f'glove.6B.{EMBEDDING_DIM}d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    input_dim=len(word_index) + 1,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_QUESTION_LEN,\n",
    "    trainable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question/answer model\n",
    "\n",
    "# question\n",
    "question = Input(shape=(MAX_QUESTION_LEN,), dtype='int32')\n",
    "\n",
    "question_embedding = embedding_layer(question)\n",
    "biLSTM = Bidirectional(LSTM(50, return_sequences=True))(question_embedding)\n",
    "max_pool = MaxPool1D(10)(biLSTM)\n",
    "flat = Flatten()(max_pool)\n",
    "dense_question = Dense(10)(flat)\n",
    "\n",
    "# answer\n",
    "answer = Input(shape=(EMBEDDING_SIZE,))\n",
    "dense_answer = Dense(64, activation='relu')(answer)\n",
    "dense_answer = Dense(10, activation='relu')(answer)\n",
    "\n",
    "# combine\n",
    "shared_dense_1 = Dense(100, activation='relu')\n",
    "shared_dense_2 = Dense(50, activation='relu')\n",
    "shared_dense_3 = Dense(10, activation='tanh')\n",
    "\n",
    "dense_answer = shared_dense_1(dense_answer)\n",
    "dense_question = shared_dense_1(dense_question)\n",
    "\n",
    "dense_answer = shared_dense_2(dense_answer)\n",
    "dense_question = shared_dense_2(dense_question)\n",
    "\n",
    "dense_answer = shared_dense_3(dense_answer)\n",
    "dense_question = shared_dense_3(dense_question)\n",
    "\n",
    "# compute cosine sim - normalized dot product\n",
    "cosine_sim = dot([dense_question, dense_answer], axes=-1)\n",
    "\n",
    "# model\n",
    "qa_model = Model([question, answer], [cosine_sim], name='qa_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrastive model\n",
    "correct_answer = Input(shape=(EMBEDDING_SIZE,))\n",
    "incorrect_answer = Input(shape=(EMBEDDING_SIZE,))\n",
    "correct_cos_sim = qa_model([question, correct_answer])\n",
    "incorrect_cos_sim = qa_model([question, incorrect_answer])\n",
    "\n",
    "def max_margin_loss(cos_sims, margin=.2):\n",
    "    correct, incorrect = cos_sims\n",
    "    # differentiable\n",
    "    return K.relu(margin - correct + incorrect)\n",
    "\n",
    "contrastive_loss = Lambda(max_margin_loss)([correct_cos_sim, incorrect_cos_sim])\n",
    "\n",
    "# model\n",
    "contrastive_model = Model([question, correct_answer, incorrect_answer], [contrastive_loss], name='contrastive_model')\n",
    "\n",
    "# prediction_model\n",
    "prediction_model = Model([question, answer], qa_model([question, answer]), name='prediction_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile\n",
    "optimizer = SGD(.001, clipnorm=1.)\n",
    "\n",
    "contrastive_model.compile(loss=lambda y_true, y_pred: y_pred, optimizer=optimizer)\n",
    "prediction_model.compile(loss=lambda y_true, y_pred: y_pred, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVG(model_to_dot(qa_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3998 samples, validate on 1002 samples\n",
      "Epoch 1/5\n",
      "3998/3998 [==============================] - 30s - loss: 0.1960 - val_loss: 0.1718\n",
      "Epoch 2/5\n",
      "3998/3998 [==============================] - 13s - loss: 0.1459 - val_loss: 0.1210\n",
      "Epoch 3/5\n",
      "3998/3998 [==============================] - 13s - loss: 0.1158 - val_loss: 0.1149\n",
      "Epoch 4/5\n",
      "3998/3998 [==============================] - 13s - loss: 0.1142 - val_loss: 0.1146\n",
      "Epoch 5/5\n",
      "3998/3998 [==============================] - 12s - loss: 0.1141 - val_loss: 0.1145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bdbdf9b0>"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrastive_model.fit(\n",
    "    x=[questions_train, correct_answers_train, incorrect_answers_train],\n",
    "    y=y_train_dummy,\n",
    "    batch_size=64,\n",
    "    epochs=5,\n",
    "    validation_data=([questions_val, correct_answers_val, incorrect_answers_val], y_val_dummy)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@united Flight is awful only one lavatory functioning, and people lining up, bumping, etc. because can't use 1st class bathroom. Ridiculous\""
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = questions[1402]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_seq = question_seqs[4]\n",
    "\n",
    "q_input = np.tile(q_seq, (3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cos_sim = prediction_model.predict([q_input, encodings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12019275],\n",
       "       [-0.07166155],\n",
       "       [ 0.06106489]], dtype=float32)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_encodings = encoder.predict(emojis).astype(np.float64)\n",
    "inf_mask = np.isinf(full_encodings).any(axis=1)\n",
    "full_encodings = full_encodings[~inf_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_input = np.tile(q_seq, (len(full_encodings), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_cos_sim = prediction_model.predict([q_input, full_encodings]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = np.argpartition(-pred_cos_sim, 4)\n",
    "result_args = pred_cos_sim[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51668197"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cos_sim.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([132, 456, 273,  86, 263])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cos_sim.argsort()[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51668197"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cos_sim[263]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACQAAAAkCAYAAADhAJiYAAADxUlEQVR4nMWY327aVhzHP3ZsoINq\n3pJNU9Qgs2lZM6laUNW7SAsPMDXdC4w8QckTQJ8g6RPAA1RT+wTjfpMWKRfp/mhFSYQmtd0ciagm\nOHgX59jYjg12oNr3Bjg+Pufj7+/r42MUFiz32X0T2AG+BTYBM3BYmXW+5j67vwgOQ0I8lhBBdYGn\nQC/NQNoCYFoSxIg5tgccZBlsHqBtoE24JEHtAp2sg94MSFvaJ6c3yOnh9ssR2JcwHj+5CUx2IHPV\nZOS00bXtpC6j/hmXf/3xDcLBblYgZXBQStWxuFWrA/vEZwXXcRgeH3F1bgGgFkvkv/q6q35QfARY\nqYHcX75L028faCQdHF8MGP5+zPhigKJp6OUK+uodAJy/+9bwz99qwGEqoFkOFbdqLaCZdNx1HN4d\n/oxr2yiFAoWNe6jFUtQxC0gFNTVDskyJMAD20a+4to1aLFG4V0XRNMYXA+zjIwGpaeira4ZeNn8C\nqsSvR14Fulpxq5Y0lyk7Jmp00vPLlF/fQNE0XMcRkI4TdmxoG6OzkzbCKV+5L9bbQF3+7E1zqE1C\ngEGUatQ/BUAvV1CLovQeTNAxeeeBuPPqyCVBxqEeGPZFElBdnpwop3+G6zgsfWj4AY5zbHTS4/Lk\nFYDoe6fcBDpLHy2bhONgAc/jgAxmlAqYuLO6BoQdy33+pVcmHya/voH26WcgorADPIwM2YH4UDcI\nlOr71mteno64u6bzY+sTAK7OLT8jS8srou2fN36bnHjizPKK3ybV5PpD+CmAGmk0EQ9KXy9PR6FP\ngLFc/LSPV/w25+1rYOIYwNXbNwDkypXoRUdhDpF3XxSoSSTId9f00CfA1fm/AKil2xPIiwEgcuJJ\nyRdQiyU/8FP0wvsSLJlJOPEAfpmCGg9tf0JPri3agpPfqj6YBeKpGwc0dQEMyps86EYKF1JJQ5So\nQYw7WZTBjanSgFdMWQDjtCg3Yscm5V43qFvVBwtzRGrb+6ISSPj/KH+RVMkQ5gXqIPJ7E7k3V4FH\n3KBsc8gC4vbcjz2g50CFjK8rc2g3ABVU3QPytCc7v091EAaAqEoQygB2oo+ODmID1XtPMNELbhEu\n3cMoEIhlvIqgtxYE8yQGxtMuk7iYs946TETY6mRcPKV6csJu2hPiHIoOuIcI/S7vhlbKcbsSpJIF\nBtK/lwn1+gbQpJBvkNNAvXY9HYzbe8xR6mxAEzATsaDWY452EK7eCEqZ8/8hk3gwC7ElPSAj2LxA\nngzEFuYHrv8900E8L7ukgFsUUFCbCLBNrr9K1ZgR8v8AtUVBbt/KkZIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=36x36 at 0x1B03335C0>"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_emoji(emojis[~inf_mask][263])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is the tweet you are using:\n",
    "- \"@united Flight is awful only one lavatory functioning, and people lining up, bumping, etc. because can't use 1st class bathroom. Ridiculous\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by training the training model, you're training everything up until, separately:\n",
    "- cosine sim between question and good answer\n",
    "- cosine sim between question and bad answer\n",
    "\n",
    "for the training model, you take it one step further by computing the hinge loss.\n",
    "\n",
    "- to predict, you need to create a model which takes a question and (candidate) answer, then computes the first bullet point above\n",
    "- again, by training the training model, you'll have trained all of the weights for this second model\n",
    "- to use it to predict, you'll just need to compile it first\n",
    "\n",
    "lavish praise on ben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# more things to try:\n",
    "- 1d convolutions on the text!\n",
    "- attention, definitely\n",
    "- we can predict on our 3 labels, and also on all emojis, and also on other emojis from the continuous space!\n",
    "- we need to play with the numerical stability of the encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# references\n",
    "- http://ben.bolte.cc/blog/2016/language.html\n",
    "- https://arxiv.org/pdf/1508.01585v2.pdf\n",
    "- https://arxiv.org/pdf/1511.04108.pdf\n",
    "- https://explosion.ai/blog/deep-learning-formula-nlp\n",
    "- https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
